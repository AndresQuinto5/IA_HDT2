{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Universidad del valle de Guatemala  \n",
        "Dpto. Ciencias de la computacion  \n",
        "Inteligencia Artificial  \n",
        "Alberto Suriano  \n",
        "\n",
        "Hoja de trabajo 2  \n",
        "Andres Quinto - 18288  \n",
        "Marlon Hernández - 15177  \n",
        "\n",
        "- Link del repositorio: https://github.com/AndresQuinto5/IA_HDT2.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1 - Preguntas Teóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
        "1. **Defina el proceso de decisión de Markov (MDP) y explique sus componentes.**  \n",
        "    Un Proceso de Decisión de Markov (MDP) es un proceso de control estocástico en tiempo discreto. Proporciona un marco matemático para modelar la toma de decisiones en situaciones en las que los resultados son en parte aleatorios y en parte bajo el control de un decisor. \n",
        "    \n",
        "    Los MDP están compuestos por un conjunto de estados S, un conjunto de acciones A, una función de transición de estados T(s,a,s’) y una función de recompensa R(s,a,s’). La función de transición de estado determina la probabilidad de pasar al estado s’ dado que el estado actual es s y la acción realizada es a. La función de recompensa da la recompensa recibida al pasar del estado s al s’ dado que la acción realizada es a.\n",
        "    \n",
        "    Referencias:  \n",
        "    [Proceso de decisión de Markov (MDP)](https://techlib.net/techedu/proceso-de-decision-de-markov-mdp/)  \n",
        "\n",
        "2. **Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas en el contexto de los PDM.**  \n",
        "    En el contexto de los MDP, la política es un mapeo de estados a acciones que maximiza la suma esperada de recompensas. La **evaluación de políticas** es el proceso de determinar el valor de una **política**, es decir, la suma total de recompensas que se espera obtener al seguir esa política. La mejora de políticas es el proceso de encontrar una nueva política que sea mejor que la política actual, basándose en los valores calculados durante la evaluación de políticas. La iteración de políticas es un método para resolver MDPs que alterna entre la evaluación de políticas y la mejora de políticas hasta que se encuentra una política óptima.\n",
        "    \n",
        "    [Políticas públicas](https://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1870-00632013000100003)\n",
        "3. **Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?**  \n",
        "    El factor de descuento (gamma) en los MDP es un parámetro que determina la importancia de las recompensas futuras en comparación con las recompensas inmediatas. Un factor de descuento cercano a 0 hace que el agente sea “miopico”, es decir, se enfoca principalmente en las recompensas inmediatas. Por otro lado, un factor de descuento cercano a 1 hace que el agente sea “visionario”, es decir, valora más las recompensas a largo plazo. Por lo tanto, el factor de descuento influye en la toma de decisiones al equilibrar la importancia de las recompensas a corto y largo plazo.\n",
        "    [Ciencias](https://www.i-ciencias.com/pregunta/183822/comprender-el-papel-del-factor-de-descuento-en-el-aprendizaje-por-refuerzo#google_vignette)  \n",
        "4. **Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.**  \n",
        "    Los algoritmos de iteración de valores y de iteración de políticas son dos métodos comunes para resolver MDPs. Ambos métodos buscan encontrar la política óptima, es decir, la política que maximiza la suma esperada de recompensas descontadas a lo largo del tiempo. La principal diferencia entre estos dos métodos radica en su enfoque:\n",
        "\n",
        "    - La iteración de valores estima los valores de los estados y usa estos valores para mejorar la política actual.  \n",
        "    - La iteración de políticas, por otro lado, evalúa y mejora la política actual directamente.  \n",
        "\n",
        "    [Factor de descuento](https://www.yubrain.com/economia/definicion-de-factor-de-descuento/#google_vignette)\n",
        "\n",
        "5. **¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala? Discuta los enfoques potenciales para abordar estos desafíos.**  \n",
        "    Resolver Procesos de Decisión de Markov (MDP) a gran escala puede presentar varios desafíos y limitaciones:\n",
        "\n",
        "    1. **La maldición de la dimensionalidad:** A medida que el número de estados y acciones en un MDP aumenta, el tiempo y el espacio necesarios para resolver el MDP crecen exponencialmente. Este problema se conoce como la “maldición de la dimensionalidad”.\n",
        "    2. **Incertidumbre en las transiciones y recompensas:** En muchos casos, las funciones de transición y recompensa no se conocen con precisión, lo que puede dificultar la resolución del MDP.\n",
        "    3. **Complejidad computacional:** La resolución de MDP implica la resolución de ecuaciones de Bellman, que pueden ser computacionalmente intensivas, especialmente para MDP a gran escala.  \n",
        "    [Evaluación del aprendizaje: Dificultades y desafíos](https://educacionabierta.org/evaluacion-del-aprendizaje-dificultades-y-desafios/)\n",
        "    - **Para abordar estos desafíos, se han propuesto varios enfoques:**\n",
        "\n",
        "    1. **Aproximación de funciones:** Este enfoque implica el uso de técnicas de aprendizaje automático para aproximar la función de valor de un MDP, lo que puede reducir significativamente la cantidad de memoria y tiempo de cálculo necesarios.\n",
        "    2. **Métodos de muestreo:** Estos métodos, como el Aprendizaje por Refuerzo basado en Monte Carlo, estiman las funciones de valor y política mediante la realización de múltiples simulaciones o “muestras” del MDP.\n",
        "    3. **Descomposición de problemas:** Algunos enfoques intentan descomponer un MDP a gran escala en varios MDP más pequeños que pueden resolverse de manera más eficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2 - Preguntas Analíticas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
        "1. **Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de\n",
        "Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones\n",
        "para la toma de decisiones.**  \n",
        "\n",
        "Para utilizar correctamente un MDP se debe tener en cuenta ciertas propiedades, las cuales son:\n",
        "\n",
        "* Sin memoria: se asume que el futuro es independiente de un estado pasado, y solo dependemos del presente. Por lo que, el estado actual debe contener toda la información relevante de los estados pasados y con esta información tomar la mejor decisión futura.\n",
        "* Transiciones de estado determinadas: se asume que las probabilidades de transición entre estados son conocidas y constantes, esto resulta en un entorno controlado y predecible, lo que nos permite basar optar por la mejor transición a un estado futuro.\n",
        "\n",
        "Existen situaciones en donde estos supuestos no se cumplen del todo lo que afecta  negativamente al rendimiento de la toma de decisiones. Por ejemplo, si en situaciones donde no toda la información relevante es observable o conocida, el estado actual no puede representar fielmente el entorno, como en juegos con información oculta, esto resulta en decisiones erroneas o imprecisas. Así mismo si el entorno es dinámico o existen eventos aleatorios que alteran el entorno, hace que la probabilidad de transición ya no sea constante, y por ende una decisión tomada puede resultar en un estado no deseado.\n",
        "\n",
        "2. **Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice\n",
        "estrategias para una toma de decisiones sólida en entornos inciertos.**\n",
        "\n",
        "Al no contar con los supuestos para garantizar la correcta funcionalidad del MDP, surgen obstáculos y desfíos los cuales se pueden tomar distintas estrategias para sobrellevarlas. Algunas soluciones para aplicar son:\n",
        "\n",
        "* Aprendizaje por refuerzo: utilizar técnicas de aprendizaje automático para adaptar las políticas de decisiones conforme se obtiene más información del entorno.\n",
        "* Políticas robustas: implementar políticas que sean efectivas y generalizables para una cantidad amplia de condiciones, incluso si no son óptimas en todos los casos.\n",
        "* Exploración activa: implementar mecanismos para explorar activamente el entorno y obtener la mayor información de modo que se reduzca la incertidumbre.\n",
        "* Decisiones secuenciales y adaptativas: adaptar las decisiones en tiempo real basándose en la retroalimentación continua del entorno.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Referencias\n",
        "* https://python.plainenglish.io/understanding-markov-decision-processes-17e852cd9981\n",
        "* https://medium.com/@ngao7/markov-decision-process-basics-3da5144d3348"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['G' 'F' 'F' 'F']\n",
            " ['F' 'F' 'F' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'H' 'H']]\n",
            "Current agent position: (0, 0)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (-1, 0)\n",
            "Invalid new position: (-1, 0)\n",
            "Current agent position: (0, 0)\n",
            "Move: (1, 0)\n",
            "New position before validation: (1, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 0)\n",
            "Move: (0, -1)\n",
            "New position before validation: (0, -1)\n",
            "Invalid new position: (0, -1)\n",
            "Current agent position: (0, 0)\n",
            "Move: (0, 1)\n",
            "New position before validation: (0, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 1)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (-1, 1)\n",
            "Invalid new position: (-1, 1)\n",
            "Current agent position: (0, 1)\n",
            "Move: (1, 0)\n",
            "New position before validation: (1, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 1)\n",
            "Move: (0, -1)\n",
            "New position before validation: (0, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 1)\n",
            "Move: (0, 1)\n",
            "New position before validation: (0, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 2)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (-1, 2)\n",
            "Invalid new position: (-1, 2)\n",
            "Current agent position: (0, 2)\n",
            "Move: (1, 0)\n",
            "New position before validation: (1, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 2)\n",
            "Move: (0, -1)\n",
            "New position before validation: (0, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 2)\n",
            "Move: (0, 1)\n",
            "New position before validation: (0, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 3)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (-1, 3)\n",
            "Invalid new position: (-1, 3)\n",
            "Current agent position: (0, 3)\n",
            "Move: (1, 0)\n",
            "New position before validation: (1, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 3)\n",
            "Move: (0, -1)\n",
            "New position before validation: (0, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 3)\n",
            "Move: (0, 1)\n",
            "New position before validation: (0, 4)\n",
            "Invalid new position: (0, 4)\n",
            "Current agent position: (1, 0)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (0, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 0)\n",
            "Move: (1, 0)\n",
            "New position before validation: (2, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 0)\n",
            "Move: (0, -1)\n",
            "New position before validation: (1, -1)\n",
            "Invalid new position: (1, -1)\n",
            "Current agent position: (1, 0)\n",
            "Move: (0, 1)\n",
            "New position before validation: (1, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 1)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (0, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 1)\n",
            "Move: (1, 0)\n",
            "New position before validation: (2, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 1)\n",
            "Move: (0, -1)\n",
            "New position before validation: (1, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 1)\n",
            "Move: (0, 1)\n",
            "New position before validation: (1, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 2)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (0, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 2)\n",
            "Move: (1, 0)\n",
            "New position before validation: (2, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 2)\n",
            "Move: (0, -1)\n",
            "New position before validation: (1, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 2)\n",
            "Move: (0, 1)\n",
            "New position before validation: (1, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 3)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (0, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 3)\n",
            "Move: (1, 0)\n",
            "New position before validation: (2, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 3)\n",
            "Move: (0, -1)\n",
            "New position before validation: (1, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 3)\n",
            "Move: (0, 1)\n",
            "New position before validation: (1, 4)\n",
            "Invalid new position: (1, 4)\n",
            "Current agent position: (2, 0)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (1, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 0)\n",
            "Move: (1, 0)\n",
            "New position before validation: (3, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 0)\n",
            "Move: (0, -1)\n",
            "New position before validation: (2, -1)\n",
            "Invalid new position: (2, -1)\n",
            "Current agent position: (2, 0)\n",
            "Move: (0, 1)\n",
            "New position before validation: (2, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 1)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (1, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 1)\n",
            "Move: (1, 0)\n",
            "New position before validation: (3, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 1)\n",
            "Move: (0, -1)\n",
            "New position before validation: (2, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 1)\n",
            "Move: (0, 1)\n",
            "New position before validation: (2, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 2)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (1, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 2)\n",
            "Move: (1, 0)\n",
            "New position before validation: (3, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 2)\n",
            "Move: (0, -1)\n",
            "New position before validation: (2, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 2)\n",
            "Move: (0, 1)\n",
            "New position before validation: (2, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 3)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (1, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 3)\n",
            "Move: (1, 0)\n",
            "New position before validation: (3, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 3)\n",
            "Move: (0, -1)\n",
            "New position before validation: (2, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 3)\n",
            "Move: (0, 1)\n",
            "New position before validation: (2, 4)\n",
            "Invalid new position: (2, 4)\n",
            "Current agent position: (3, 0)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (2, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 0)\n",
            "Move: (1, 0)\n",
            "New position before validation: (4, 0)\n",
            "Invalid new position: (4, 0)\n",
            "Current agent position: (3, 0)\n",
            "Move: (0, -1)\n",
            "New position before validation: (3, -1)\n",
            "Invalid new position: (3, -1)\n",
            "Current agent position: (3, 0)\n",
            "Move: (0, 1)\n",
            "New position before validation: (3, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 1)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (2, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 1)\n",
            "Move: (1, 0)\n",
            "New position before validation: (4, 1)\n",
            "Invalid new position: (4, 1)\n",
            "Current agent position: (3, 1)\n",
            "Move: (0, -1)\n",
            "New position before validation: (3, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 1)\n",
            "Move: (0, 1)\n",
            "New position before validation: (3, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 2)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (2, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 2)\n",
            "Move: (1, 0)\n",
            "New position before validation: (4, 2)\n",
            "Invalid new position: (4, 2)\n",
            "Current agent position: (3, 2)\n",
            "Move: (0, -1)\n",
            "New position before validation: (3, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 2)\n",
            "Move: (0, 1)\n",
            "New position before validation: (3, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 3)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (2, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 3)\n",
            "Move: (1, 0)\n",
            "New position before validation: (4, 3)\n",
            "Invalid new position: (4, 3)\n",
            "Current agent position: (3, 3)\n",
            "Move: (0, -1)\n",
            "New position before validation: (3, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 3)\n",
            "Move: (0, 1)\n",
            "New position before validation: (3, 4)\n",
            "Invalid new position: (3, 4)\n",
            "Current agent position: (0, 0)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (-1, 0)\n",
            "Invalid new position: (-1, 0)\n",
            "Current agent position: (0, 0)\n",
            "Move: (1, 0)\n",
            "New position before validation: (1, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 0)\n",
            "Move: (0, -1)\n",
            "New position before validation: (0, -1)\n",
            "Invalid new position: (0, -1)\n",
            "Current agent position: (0, 0)\n",
            "Move: (0, 1)\n",
            "New position before validation: (0, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 1)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (-1, 1)\n",
            "Invalid new position: (-1, 1)\n",
            "Current agent position: (0, 1)\n",
            "Move: (1, 0)\n",
            "New position before validation: (1, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 1)\n",
            "Move: (0, -1)\n",
            "New position before validation: (0, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 1)\n",
            "Move: (0, 1)\n",
            "New position before validation: (0, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 2)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (-1, 2)\n",
            "Invalid new position: (-1, 2)\n",
            "Current agent position: (0, 2)\n",
            "Move: (1, 0)\n",
            "New position before validation: (1, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 2)\n",
            "Move: (0, -1)\n",
            "New position before validation: (0, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 2)\n",
            "Move: (0, 1)\n",
            "New position before validation: (0, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 3)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (-1, 3)\n",
            "Invalid new position: (-1, 3)\n",
            "Current agent position: (0, 3)\n",
            "Move: (1, 0)\n",
            "New position before validation: (1, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 3)\n",
            "Move: (0, -1)\n",
            "New position before validation: (0, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (0, 3)\n",
            "Move: (0, 1)\n",
            "New position before validation: (0, 4)\n",
            "Invalid new position: (0, 4)\n",
            "Current agent position: (1, 0)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (0, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 0)\n",
            "Move: (1, 0)\n",
            "New position before validation: (2, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 0)\n",
            "Move: (0, -1)\n",
            "New position before validation: (1, -1)\n",
            "Invalid new position: (1, -1)\n",
            "Current agent position: (1, 0)\n",
            "Move: (0, 1)\n",
            "New position before validation: (1, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 1)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (0, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 1)\n",
            "Move: (1, 0)\n",
            "New position before validation: (2, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 1)\n",
            "Move: (0, -1)\n",
            "New position before validation: (1, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 1)\n",
            "Move: (0, 1)\n",
            "New position before validation: (1, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 2)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (0, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 2)\n",
            "Move: (1, 0)\n",
            "New position before validation: (2, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 2)\n",
            "Move: (0, -1)\n",
            "New position before validation: (1, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 2)\n",
            "Move: (0, 1)\n",
            "New position before validation: (1, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 3)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (0, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 3)\n",
            "Move: (1, 0)\n",
            "New position before validation: (2, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 3)\n",
            "Move: (0, -1)\n",
            "New position before validation: (1, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (1, 3)\n",
            "Move: (0, 1)\n",
            "New position before validation: (1, 4)\n",
            "Invalid new position: (1, 4)\n",
            "Current agent position: (2, 0)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (1, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 0)\n",
            "Move: (1, 0)\n",
            "New position before validation: (3, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 0)\n",
            "Move: (0, -1)\n",
            "New position before validation: (2, -1)\n",
            "Invalid new position: (2, -1)\n",
            "Current agent position: (2, 0)\n",
            "Move: (0, 1)\n",
            "New position before validation: (2, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 1)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (1, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 1)\n",
            "Move: (1, 0)\n",
            "New position before validation: (3, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 1)\n",
            "Move: (0, -1)\n",
            "New position before validation: (2, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 1)\n",
            "Move: (0, 1)\n",
            "New position before validation: (2, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 2)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (1, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 2)\n",
            "Move: (1, 0)\n",
            "New position before validation: (3, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 2)\n",
            "Move: (0, -1)\n",
            "New position before validation: (2, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 2)\n",
            "Move: (0, 1)\n",
            "New position before validation: (2, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 3)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (1, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 3)\n",
            "Move: (1, 0)\n",
            "New position before validation: (3, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 3)\n",
            "Move: (0, -1)\n",
            "New position before validation: (2, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (2, 3)\n",
            "Move: (0, 1)\n",
            "New position before validation: (2, 4)\n",
            "Invalid new position: (2, 4)\n",
            "Current agent position: (3, 0)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (2, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 0)\n",
            "Move: (1, 0)\n",
            "New position before validation: (4, 0)\n",
            "Invalid new position: (4, 0)\n",
            "Current agent position: (3, 0)\n",
            "Move: (0, -1)\n",
            "New position before validation: (3, -1)\n",
            "Invalid new position: (3, -1)\n",
            "Current agent position: (3, 0)\n",
            "Move: (0, 1)\n",
            "New position before validation: (3, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 1)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (2, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 1)\n",
            "Move: (1, 0)\n",
            "New position before validation: (4, 1)\n",
            "Invalid new position: (4, 1)\n",
            "Current agent position: (3, 1)\n",
            "Move: (0, -1)\n",
            "New position before validation: (3, 0)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 1)\n",
            "Move: (0, 1)\n",
            "New position before validation: (3, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 2)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (2, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 2)\n",
            "Move: (1, 0)\n",
            "New position before validation: (4, 2)\n",
            "Invalid new position: (4, 2)\n",
            "Current agent position: (3, 2)\n",
            "Move: (0, -1)\n",
            "New position before validation: (3, 1)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 2)\n",
            "Move: (0, 1)\n",
            "New position before validation: (3, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 3)\n",
            "Move: (-1, 0)\n",
            "New position before validation: (2, 3)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 3)\n",
            "Move: (1, 0)\n",
            "New position before validation: (4, 3)\n",
            "Invalid new position: (4, 3)\n",
            "Current agent position: (3, 3)\n",
            "Move: (0, -1)\n",
            "New position before validation: (3, 2)\n",
            "[['G' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['F' 'F' 'H' 'F']\n",
            " ['S' 'F' 'F' 'F']]\n",
            "Current agent position: (3, 3)\n",
            "Move: (0, 1)\n",
            "New position before validation: (3, 4)\n",
            "Invalid new position: (3, 4)\n",
            "Optimal Policy:\n",
            "[1 2 2 2 0 0 2 0 0 0 2 1 0 0 2 2]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from gym import spaces\n",
        "\n",
        "class FrozenLakeEnv():\n",
        "    \"\"\"FrozenLakeEnv class.\n",
        "\n",
        "Implements a Frozen Lake environment with discrete actions and observations.\n",
        "The environment is a 4x4 grid with holes ('H'), the agent's start position ('S'),\n",
        "a goal position ('G'), and frozen tiles ('F'). \n",
        "\n",
        "The agent can take 4 actions (up, down, left, right) and gets reward +1 for \n",
        "reaching the goal, reward -1 for falling in a hole, and reward -0.01 otherwise.\n",
        "\n",
        "Has `step()`, `reset()`, `render()` methods to interact with the environment.\n",
        "\"\"\"\n",
        "    def __init__(self):\n",
        "        self.action_space = spaces.Discrete(4)  # Movimientos posibles (arriba, abajo, derecha, izquierda)\n",
        "        self.observation_space = spaces.Discrete(16)  # Matriz de 4x4 con 16 estados posibles\n",
        "        \n",
        "        # Inicializar el mapa del hielo congelado y la posición inicial del agente\n",
        "        self.map = np.zeros((4, 4), dtype=str)\n",
        "        self.map = np.full((4, 4), 'F', dtype=str)\n",
        "        self.start_pos = tuple()\n",
        "        self.goal_pos = tuple()\n",
        "        \n",
        "        # Generar la posición inicial del agente en una esquina aleatoria\n",
        "        corner_indices = [(0, 0), (0, 3), (3, 0), (3, 3)]\n",
        "        start_corner_idx = np.random.choice(len(corner_indices))\n",
        "        self.start_pos = corner_indices[start_corner_idx]\n",
        "        \n",
        "        # Generar la posición de la meta en la esquina opuesta\n",
        "        opposite_corner_idx = (start_corner_idx + 2) % 4\n",
        "        self.goal_pos = corner_indices[opposite_corner_idx]\n",
        "        \n",
        "        # Establecer la posición inicial del agente y la meta en las coordenadas generadas\n",
        "        self.map[self.start_pos] = 'S'\n",
        "        self.map[self.goal_pos] = 'G'\n",
        "        self.agent_pos = np.array(self.start_pos, dtype=int)\n",
        "        \n",
        "        # Crear agujeros aleatorios que no estén adyacentes a la meta, asegurandome que ahora los agujeros no se crean en las posiciones de la meta o el agente\n",
        "\n",
        "        for _ in range(3):\n",
        "            hole_pos = np.random.randint(0, 4, size=2)\n",
        "            while self._is_adjacent_to_goal(hole_pos) or tuple(hole_pos) == self.start_pos or tuple(hole_pos) == self.goal_pos:\n",
        "                hole_pos = np.random.randint(0, 4, size=2)\n",
        "            self.map[tuple(hole_pos)] = 'H'\n",
        "\n",
        "    def _is_adjacent_to_goal(self, pos):\n",
        "        # Verificar si la posición dada está adyacente a la meta\n",
        "        for move in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
        "            new_pos = tuple(np.array(pos) + np.array(move))\n",
        "            if new_pos == self.goal_pos:\n",
        "                return True\n",
        "        return False\n",
        "        \n",
        "    def step(self, action):\n",
        "        # Move the agent according to the given action and calculate the new position\n",
        "        move = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}[action]\n",
        "        new_pos = (self.agent_pos[0] + move[0], self.agent_pos[1] + move[1])\n",
        "        \n",
        "        print(\"Current agent position:\", self.agent_pos)\n",
        "        print(\"Move:\", move)\n",
        "        print(\"New position before validation:\", new_pos)\n",
        "        \n",
        "        # Check if the new position is within the valid range\n",
        "        if not (0 <= new_pos[0] < self.map.shape[0] and 0 <= new_pos[1] < self.map.shape[1]):\n",
        "            print(\"Invalid new position:\", new_pos)\n",
        "            # If the new position is invalid, return the current position and a negative reward\n",
        "            return self.agent_pos, -1, True, {}\n",
        "        \n",
        "        # Calculate the reward and 'done' flag based on the new position\n",
        "        if self.map[new_pos] == 'H':\n",
        "            reward = -1\n",
        "            done = True\n",
        "        elif self.map[new_pos] == 'G':\n",
        "            reward = 1\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -0.01\n",
        "            done = False\n",
        "        \n",
        "        # Update the agent's position\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        self.render()\n",
        "        \n",
        "        return new_pos, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        # Reiniciar el entorno y poner al agente en la posición inicial\n",
        "        self.agent_pos = np.array(self.start_pos, dtype=int)\n",
        "        return tuple(self.agent_pos)\n",
        "    \n",
        "    def render(self):\n",
        "        print(self.map)\n",
        "\n",
        "\n",
        "class FrozenLakeMDP():\n",
        "    \"\"\"\n",
        "    Represents a Markov Decision Process (MDP) for the FrozenLake environment.\n",
        "\n",
        "    Attributes:\n",
        "        env (FrozenLakeEnv): The FrozenLake environment.\n",
        "        num_states (int): The number of states in the environment.\n",
        "        num_actions (int): The number of actions in the environment.\n",
        "        transition_matrix (ndarray): The transition matrix representing the probabilities of transitioning from one state to another given an action.\n",
        "        reward_matrix (ndarray): The reward matrix representing the rewards obtained for transitioning from one state to another given an action.\n",
        "        discount_factor (float): The discount factor for future rewards.\n",
        "\n",
        "    Methods:\n",
        "        __init__(self, seed): Initializes the FrozenLakeMDP object.\n",
        "        _build_transition_matrix(self): Builds the transition matrix.\n",
        "        _build_reward_matrix(self): Builds the reward matrix.\n",
        "        value_iteration(self, epsilon=1e-8): Performs value iteration to find the optimal value function.\n",
        "        policy_extraction(self, values): Extracts the optimal policy based on the given values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, seed):\n",
        "        np.random.seed(seed)\n",
        "        self.env = FrozenLakeEnv()\n",
        "        self.num_states = self.env.observation_space.n\n",
        "        self.num_actions = self.env.action_space.n\n",
        "        self.transition_matrix = self._build_transition_matrix()\n",
        "        self.reward_matrix = self._build_reward_matrix()\n",
        "        self.discount_factor = 0.99\n",
        "\n",
        "    def _build_transition_matrix(self):\n",
        "        \"\"\"\n",
        "        Builds the transition matrix.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: The transition matrix.\n",
        "        \"\"\"\n",
        "        transition_matrix = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
        "        for state in range(self.num_states):\n",
        "            for action in range(self.num_actions):\n",
        "                self.env.reset()\n",
        "                self.env.agent_pos = np.unravel_index(state, self.env.map.shape)\n",
        "                new_state, _, _, _ = self.env.step(action)\n",
        "                new_state_idx = np.ravel_multi_index(new_state, self.env.map.shape)\n",
        "                transition_matrix[state, action, new_state_idx] = 1\n",
        "        return transition_matrix\n",
        "\n",
        "    def _build_reward_matrix(self):\n",
        "        \"\"\"\n",
        "        Builds the reward matrix.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: The reward matrix.\n",
        "        \"\"\"\n",
        "        reward_matrix = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
        "        for state in range(self.num_states):\n",
        "            for action in range(self.num_actions):\n",
        "                self.env.reset()\n",
        "                self.env.agent_pos = np.unravel_index(state, self.env.map.shape)\n",
        "                _, reward, _, _ = self.env.step(action)\n",
        "                reward_matrix[state, action, :] = reward\n",
        "        return reward_matrix\n",
        "\n",
        "    def value_iteration(self, epsilon=1e-8):\n",
        "        \"\"\"\n",
        "        Performs value iteration to find the optimal value function.\n",
        "\n",
        "        Args:\n",
        "            epsilon (float, optional): The convergence threshold. Defaults to 1e-8.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: The optimal value function.\n",
        "        \"\"\"\n",
        "        values = np.zeros(self.num_states)\n",
        "        while True:\n",
        "            prev_values = np.copy(values)\n",
        "            values = np.max(np.sum(self.transition_matrix * (self.reward_matrix + self.discount_factor * prev_values), axis=2), axis=1)\n",
        "            if np.max(np.abs(values - prev_values)) < epsilon:\n",
        "                break\n",
        "        return values\n",
        "\n",
        "    def policy_extraction(self, values):\n",
        "        \"\"\"\n",
        "        Extracts the optimal policy based on the given values.\n",
        "\n",
        "        Args:\n",
        "            values (ndarray): The optimal value function.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: The optimal policy.\n",
        "        \"\"\"\n",
        "        policy = np.argmax(np.sum(self.transition_matrix * (self.reward_matrix + self.discount_factor * values), axis=2), axis=1)\n",
        "        return policy\n",
        "\n",
        "env = FrozenLakeEnv()\n",
        "env.render()\n",
        "\n",
        "# Create an instance of the FrozenLakeMDP class\n",
        "mdp = FrozenLakeMDP(seed=42)\n",
        "\n",
        "# Perform value iteration to calculate the optimal state values\n",
        "values = mdp.value_iteration()\n",
        "\n",
        "# Extract the optimal policy based on the calculated state values\n",
        "policy = mdp.policy_extraction(values)\n",
        "\n",
        "# Print the optimal policy\n",
        "print(\"Optimal Policy:\")\n",
        "print(policy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explicación del Algoritmo\n",
        "\n",
        "El resultado en la consola muestra la política óptima que ha sido calculada para el entorno Frozen Lake. La política óptima determina qué acción debe tomar el agente en cada estado para maximizar su recompensa acumulada esperada.\n",
        "\n",
        "La política óptima se muestra como una lista de números, donde cada número representa la acción a tomar en cada estado:\n",
        "\n",
        "- 0: Arriba\n",
        "- 1: Abajo\n",
        "- 2: Izquierda\n",
        "- 3: Derecha\n",
        "\n",
        "#### Interpretación de la Política:\n",
        "\n",
        "La lista `[1 2 2 2 0 0 2 0 0 0 2 1 0 0 2 2]` tiene 16 elementos, correspondientes a los 16 estados del entorno Frozen Lake.\n",
        "\n",
        "- Estado 0: Acción 1 (Abajo)\n",
        "- Estado 1: Acción 2 (Izquierda)\n",
        "- Estado 2: Acción 2 (Izquierda)\n",
        "- Estado 3: Acción 2 (Izquierda)\n",
        "- Estado 4: Acción 0 (Arriba)\n",
        "- Estado 5: Acción 0 (Arriba)\n",
        "- Estado 6: Acción 2 (Izquierda)\n",
        "- Estado 7: Acción 0 (Arriba)\n",
        "- Estado 8: Acción 0 (Arriba)\n",
        "- Estado 9: Acción 0 (Arriba)\n",
        "- Estado 10: Acción 2 (Izquierda)\n",
        "- Estado 11: Acción 1 (Abajo)\n",
        "- Estado 12: Acción 0 (Arriba)\n",
        "- Estado 13: Acción 0 (Arriba)\n",
        "- Estado 14: Acción 2 (Izquierda)\n",
        "- Estado 15: Acción 2 (Izquierda)\n",
        "\n",
        "#### Visualización en el Mapa Inicial:\n",
        "\n",
        "El mapa inicial del entorno Frozen Lake es el siguiente:\n",
        "\n",
        "```\n",
        "[['G' 'F' 'F' 'F']\n",
        "['F' 'F' 'F' 'F']\n",
        "['F' 'F' 'H' 'F']\n",
        "['S' 'F' 'H' 'H']]\n",
        "```\n",
        "\n",
        "\n",
        "- El agente comienza en la posición marcada como 'S' (Estado 12).\n",
        "- Según la política óptima, el agente debe moverse hacia arriba desde el Estado 12 para llegar al Estado 8.\n",
        "- Desde el Estado 8, el agente debe moverse nuevamente hacia arriba para llegar al Estado 4.\n",
        "- Luego, desde el Estado 4, debe moverse hacia arriba para llegar al Estado 0, que es el estado objetivo 'G'.\n",
        "\n",
        "Por lo tanto, la política óptima guía al agente a moverse hacia arriba hasta que llegue al estado objetivo, evitando los agujeros ('H') tanto como sea posible.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "0.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
