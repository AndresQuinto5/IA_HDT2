{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Universidad del valle de Guatemala  \n",
        "Dpto. Ciencias de la computacion  \n",
        "Inteligencia Artificial  \n",
        "Alberto Suriano  \n",
        "\n",
        "Hoja de trabajo 2  \n",
        "Andres Quinto - 18288  \n",
        "Marlon Hernández - 15177  \n",
        "\n",
        "- Link del repositorio: https://github.com/AndresQuinto5/IA_HDT2.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1 - Preguntas Teóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
        "1. **Defina el proceso de decisión de Markov (MDP) y explique sus componentes.**  \n",
        "    Un Proceso de Decisión de Markov (MDP) es un proceso de control estocástico en tiempo discreto. Proporciona un marco matemático para modelar la toma de decisiones en situaciones en las que los resultados son en parte aleatorios y en parte bajo el control de un decisor. \n",
        "    \n",
        "    Los MDP están compuestos por un conjunto de estados S, un conjunto de acciones A, una función de transición de estados T(s,a,s’) y una función de recompensa R(s,a,s’). La función de transición de estado determina la probabilidad de pasar al estado s’ dado que el estado actual es s y la acción realizada es a. La función de recompensa da la recompensa recibida al pasar del estado s al s’ dado que la acción realizada es a.\n",
        "    \n",
        "    Referencias:  \n",
        "    [Proceso de decisión de Markov (MDP)](https://techlib.net/techedu/proceso-de-decision-de-markov-mdp/)  \n",
        "\n",
        "2. **Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas en el contexto de los PDM.**  \n",
        "    En el contexto de los MDP, la política es un mapeo de estados a acciones que maximiza la suma esperada de recompensas. La **evaluación de políticas** es el proceso de determinar el valor de una **política**, es decir, la suma total de recompensas que se espera obtener al seguir esa política. La mejora de políticas es el proceso de encontrar una nueva política que sea mejor que la política actual, basándose en los valores calculados durante la evaluación de políticas. La iteración de políticas es un método para resolver MDPs que alterna entre la evaluación de políticas y la mejora de políticas hasta que se encuentra una política óptima.\n",
        "    \n",
        "    [Políticas públicas](https://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1870-00632013000100003)\n",
        "3. **Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?**  \n",
        "    El factor de descuento (gamma) en los MDP es un parámetro que determina la importancia de las recompensas futuras en comparación con las recompensas inmediatas. Un factor de descuento cercano a 0 hace que el agente sea “miopico”, es decir, se enfoca principalmente en las recompensas inmediatas. Por otro lado, un factor de descuento cercano a 1 hace que el agente sea “visionario”, es decir, valora más las recompensas a largo plazo. Por lo tanto, el factor de descuento influye en la toma de decisiones al equilibrar la importancia de las recompensas a corto y largo plazo.\n",
        "    [Ciencias](https://www.i-ciencias.com/pregunta/183822/comprender-el-papel-del-factor-de-descuento-en-el-aprendizaje-por-refuerzo#google_vignette)  \n",
        "4. **Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.**  \n",
        "    Los algoritmos de iteración de valores y de iteración de políticas son dos métodos comunes para resolver MDPs. Ambos métodos buscan encontrar la política óptima, es decir, la política que maximiza la suma esperada de recompensas descontadas a lo largo del tiempo. La principal diferencia entre estos dos métodos radica en su enfoque:\n",
        "\n",
        "    - La iteración de valores estima los valores de los estados y usa estos valores para mejorar la política actual.  \n",
        "    - La iteración de políticas, por otro lado, evalúa y mejora la política actual directamente.  \n",
        "\n",
        "    [Factor de descuento](https://www.yubrain.com/economia/definicion-de-factor-de-descuento/#google_vignette)\n",
        "\n",
        "5. **¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala? Discuta los enfoques potenciales para abordar estos desafíos.**  \n",
        "    Resolver Procesos de Decisión de Markov (MDP) a gran escala puede presentar varios desafíos y limitaciones:\n",
        "\n",
        "    1. **La maldición de la dimensionalidad:** A medida que el número de estados y acciones en un MDP aumenta, el tiempo y el espacio necesarios para resolver el MDP crecen exponencialmente. Este problema se conoce como la “maldición de la dimensionalidad”.\n",
        "    2. **Incertidumbre en las transiciones y recompensas:** En muchos casos, las funciones de transición y recompensa no se conocen con precisión, lo que puede dificultar la resolución del MDP.\n",
        "    3. **Complejidad computacional:** La resolución de MDP implica la resolución de ecuaciones de Bellman, que pueden ser computacionalmente intensivas, especialmente para MDP a gran escala.  \n",
        "    [Evaluación del aprendizaje: Dificultades y desafíos](https://educacionabierta.org/evaluacion-del-aprendizaje-dificultades-y-desafios/)\n",
        "    - **Para abordar estos desafíos, se han propuesto varios enfoques:**\n",
        "\n",
        "    1. **Aproximación de funciones:** Este enfoque implica el uso de técnicas de aprendizaje automático para aproximar la función de valor de un MDP, lo que puede reducir significativamente la cantidad de memoria y tiempo de cálculo necesarios.\n",
        "    2. **Métodos de muestreo:** Estos métodos, como el Aprendizaje por Refuerzo basado en Monte Carlo, estiman las funciones de valor y política mediante la realización de múltiples simulaciones o “muestras” del MDP.\n",
        "    3. **Descomposición de problemas:** Algunos enfoques intentan descomponer un MDP a gran escala en varios MDP más pequeños que pueden resolverse de manera más eficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2 - Preguntas Analíticas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
        "1. Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de\n",
        "Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones\n",
        "para la toma de decisiones.\n",
        "2. Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice\n",
        "estrategias para una toma de decisiones sólida en entornos inciertos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "0.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
